<!-- About Section -->
<section id="about" class="about">
    <div class="content-wrapper">
        <div class="col-lg-10 col-lg-offset-1">
            <h1>Call For Papers</h1>
            <div class="row about-details">
                <p align="justify">
                    <big>Virtual embodied agents and robots are increasingly integrated into our daily lives, serving roles such as receptionists in public services, home assistants, virtual personal trainers, and coaches for physical and mental health activities. To ensure user acceptance and trust, it is essential to design these agents to be not only functionally complex and useful but also understandable and socially appropriate. People are more likely to accept such technologies when they perceive them as extensions of themselves. Therefore, human-human interaction, which incorporates not only language but also non-verbal communication that is richer in terms of social and cultural features, serves as a natural model for designing the behaviours of such agents. Consequently, there has been a significant effort in recent years to generate non-verbal gestures for agents automatically in a data-driven manner.</big>
                </p>
                <p align="justify">
                    <big>This workshop aims to advance the development of real-world applications that involve virtual embodied agents and robots. To ensure user acceptance and trust, such real-world applications require new generative models and evaluation methods to generate socially appropriate non-verbal behaviours, considering cultural and personal factors, as well as enable real-time processing (i.e., understanding and responding to the user on the fly).</big>
                </p>
                <p align="justify"> 
                    <big><b><big>TOPICS</big></b><br>
                    The main suggested topics for the workshop include, but are not limited to:
                    </big>
                </p>
                <big>
                    <ul style="list-style-position: inside;">
                        <li style="text-align: left;">Multimodal (i.e., vision, audio, and/or text)data processing for gesture generation</li>
                        <li style="text-align: left;">Multimodal data modelling for understanding personal, affective, and cultural factors Real-time gesture generation techniques</li>
                        <li style="text-align: left;">Real-time gesture generation techniques</li>
                        <li style="text-align: left;">New generative models to model personal factors (e.g., culture, affect, and personality)</li>
                        <li style="text-align: left;">Incorporating personal factors into gesture generation</li>
                        <li style="text-align: left;">Gesture generation beyond monologues, in multi-party interaction settings </li>
                        <li style="text-align: left;">New approaches to transfer human-human interactions to human-agent interactions</li>
                        <li style="text-align: left;">Transfer learning and transformer techniques to apply virtual agents' gestures to reduced degrees of freedom embodied agents</li>
                        <li style="text-align: left;">Multimodal  data modelling techniques for detecting human trust and openness towards embodied agents</li>
                    </ul>
                </big>
                <!-- <p align="justify">
                <big>The Expressive Encounters Workshop is organized by the members of ... .</big>
                </p> -->
            </div>
        </div>
    </div>
</section>
<!-- End About Section -->
